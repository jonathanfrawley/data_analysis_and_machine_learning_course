{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises 03: Deep Learning\n",
    "*Link to slides: <https://bit.ly/2Vqx3kO>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note: Enabling GPU\n",
    "To make sure this is enabled, go to the \"Runtime\" menu at the top of the page, and click select the \"Change Runtime Type\" option. Under \"Hardware Accelerator\", choose \"GPU\" and then hit \"Save\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyZsovv41X-S"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRzLMnMv6SU7"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 1000\n",
    "LEARNING_RATE = 1\n",
    "GAMMA = 0.7\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "cpB1ZaE26Wc-",
    "outputId": "88befa84-f99e-4ddf-8538-f6c0428fe23e"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "print(\"#################################\")\n",
    "print(\"# Input:\")\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "test_iter = iter(test_loader)\n",
    "images, labels = train_iter.next()\n",
    "print(f'Images is tensor of shape: {images.shape}, labels is tensor of shape: {labels.shape}')\n",
    "figure = plt.figure()\n",
    "num_of_images = 64\n",
    "for index in range(1, num_of_images + 1):\n",
    "    plt.subplot(8, 8, index)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(images[index-1].numpy().squeeze(), cmap='gray_r')\n",
    "print(\"Images:\")\n",
    "plt.show()\n",
    "print(\"Labels:\")\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        print(labels[i*8+j].item(), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5GUg1ib2VXS"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ho8eVTZf48uW"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "colab_type": "code",
    "id": "cmmj0eCu5nPk",
    "outputId": "32b044e6-cb9b-4388-c65c-54476e1bb66c"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "ITER_PER_EPOCH = 1000\n",
    "TEST_ITER_PER_EPOCH = len(test_loader)\n",
    "LOG_INTERVAL = 100\n",
    "\n",
    "print(\"#################################\")\n",
    "print(\"# Training:\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_iter = itertools.cycle(iter(train_loader))\n",
    "    test_iter =  iter(test_loader)\n",
    "\n",
    "    for batch_idx in range(ITER_PER_EPOCH):\n",
    "        (data, target) = next(train_iter)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    images = None\n",
    "    preds = None\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(TEST_ITER_PER_EPOCH):\n",
    "            (data, target) = next(test_iter)\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += data.shape[0]\n",
    "            images = data.cpu()\n",
    "            preds = pred.cpu().numpy()\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, total,\n",
    "            100. * correct / total))\n",
    "\n",
    "    print(f'Images is tensor of shape: {images.shape}, preds is tensor of shape: {preds.shape}')\n",
    "    figure = plt.figure()\n",
    "    num_of_images = 64\n",
    "    for index in range(num_of_images):\n",
    "        plt.subplot(8, 8, index+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n",
    "    print(\"#################################\")\n",
    "    print(\"# Test Output:\")\n",
    "\n",
    "    print(\"Images:\")\n",
    "    plt.show()\n",
    "    print(\"Preds:\")\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            print(preds[i*8+j].item(), end=',')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3AeF0bTT7j0"
   },
   "source": [
    "## Exercise 1\n",
    " - Change the above to use [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) as an optimizer\n",
    " - Retrain the model\n",
    " - How does the accuracy change? Can you improve accuracy by modifying the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZDGYtIqUNGX"
   },
   "source": [
    "## Exercise 2\n",
    " - Add a plot which shows loss over time\n",
    " - Add a second plot which shows test accuracy over time (you can use (correct / total) for your accuracy score\n",
    " - Set EPOCHS to 10 and rerun to see how test accuracy increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xuLQATzHUYu8"
   },
   "source": [
    "## Exercise 3\n",
    "Try increase / reduce the amount of dropout on the model. How does this affect the accuracy? "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "exercises_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
